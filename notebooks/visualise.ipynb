{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import sys\n",
    "sys.path.append(\"/root/clip\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn as nn\n",
    "import torchaudio.transforms as atrans\n",
    "from clipmbt.data.data_loading import load_data, Collate_Constrastive\n",
    "from clipmbt.datasets import tess\n",
    "from clipmbt.constants import *\n",
    "from clipmbt.tokenizer import make_audio_input\n",
    "\n",
    "\n",
    "ds_to_use = tess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def image_grid(batch1=None, batch2=None):\n",
    "    assert (batch1 is not None) or (batch2 is not None), \"rgb_batch or spec_batch must be present, cannot both be empty!\"\n",
    "    batch1_img_grid = None\n",
    "    batch2_img_grid = None\n",
    "    if batch1 is not None:\n",
    "        batch_sz, height, width = batch1.shape\n",
    "        spec_copy = batch1.clone().permute(0, 1, 2)\n",
    "        batch1_img_grid = spec_copy.reshape(height*batch_sz, width)\n",
    "\n",
    "    if batch2 is not None:\n",
    "        batch_sz, height, width = batch2.shape\n",
    "        spec_copy = batch2.clone().permute(0, 1, 2)\n",
    "        batch2_img_grid = spec_copy.reshape(height*batch_sz, width)\n",
    "    \n",
    "    if (batch1_img_grid is not None) and (batch2_img_grid is not None):\n",
    "        img_grid = torch.cat((batch1_img_grid, batch2_img_grid), dim=1)\n",
    "    else:\n",
    "        img_grid = batch1_img_grid if batch1_img_grid is not None else batch2_img_grid\n",
    "    return img_grid\n",
    "\n",
    "\n",
    "def show_batch(batch_1=None, batch_2=None, title=\"Image batch\", size=5):\n",
    "    result = image_grid(batch_1, batch_2)\n",
    "    fig = plt.figure(figsize=(size, size))\n",
    "    plt.suptitle(f\"{title}\")\n",
    "    plt.imshow(result, interpolation=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hop_len = int(ds_to_use.SAMPLING_RATE * ds_to_use.SPEC_HOP_LEN_MS / 1000)\n",
    "win_len = int(ds_to_use.SAMPLING_RATE * ds_to_use.SPEC_WINDOW_SZ_MS/ 1000)\n",
    "audio_transform = nn.Sequential(\n",
    "    atrans.MelSpectrogram(\n",
    "        sample_rate=ds_to_use.SAMPLING_RATE,\n",
    "        n_fft=1024,\n",
    "        n_mels=NUM_MELS,\n",
    "        win_length=win_len,\n",
    "        hop_length=hop_len,\n",
    "        normalized=True,\n",
    "        pad_mode=\"constant\"\n",
    "    ),\n",
    "    atrans.AmplitudeToDB()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/intelpa-1/datasets/tess_dataset/OAF_ps/OAF_fail_ps.wav\n"
     ]
    }
   ],
   "source": [
    "from clipmbt.data.utils import invert_melspec\n",
    "import torchaudio\n",
    "from pathlib import Path\n",
    "import librosa\n",
    "import scipy\n",
    "\n",
    "emotion = \"ps\"\n",
    "words = [\"fail\", \"judge\", \"nag\", \"search\", \"youth\", \"late\", \"laud\", \"ring\", \"red\", \"shall\", \"gaze\"]\n",
    "of_files = [f\"{ds_to_use.DATA_DIR}/OAF_{emotion}/OAF_{word}_{emotion.lower()}.wav\" for word in words]\n",
    "yf_files = [f\"{ds_to_use.DATA_DIR}/YAF_{emotion}/YAF_{word}_{emotion.lower()}.wav\" for word in words]\n",
    "\n",
    "of_signals = [make_audio_input(of_file) for of_file in of_files]\n",
    "yf_signals = [make_audio_input(yf_file) for yf_file in yf_files]\n",
    "\n",
    "of_specs = pad_sequence([audio_transform(of_signal).permute(1, 0) for of_signal in of_signals]).permute(1, 2, 0)\n",
    "yf_specs = pad_sequence([audio_transform(yf_signal).permute(1, 0) for yf_signal in yf_signals]).permute(1, 2, 0)\n",
    "\n",
    "# of_tmp = of_specs + abs(of_specs.min())\n",
    "# of_norm = of_tmp / of_tmp.max()\n",
    "# yf_tmp = yf_specs + abs(yf_specs.min())\n",
    "# yf_norm = yf_tmp / yf_tmp.max()\n",
    "\n",
    "\n",
    "\n",
    "test_inv_file = Path(of_files[0])\n",
    "print(test_inv_file)\n",
    "n_fft = 1024\n",
    "hop_length = int(ds_to_use.SAMPLING_RATE * ds_to_use.SPEC_HOP_LEN_MS / 1000)\n",
    "mspec = of_specs[0]\n",
    "\n",
    "audio_signal = invert_melspec(mspec, n_fft=n_fft, sr=ds_to_use.SAMPLING_RATE, hop_len=hop_len)\n",
    "\n",
    "torchaudio.save(f\"./{test_inv_file.stem}_reconstructed.wav\", audio_signal, ds_to_use.SAMPLING_RATE)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(test_inv_file)\n",
    "# test_inv_spec = of_specs[0]\n",
    "# print(test_inv_spec.shape)\n",
    "# test_inv_signal = invert_melspec(test_inv_spec, n_fft=513, sr=ds_to_use.SAMPLING_RATE)\n",
    "# torchaudio.save(f\"./{test_inv_file.with_suffix('')}_reconstructed.wav\")\n",
    "\n",
    "\n",
    "# print(of_specs_ampl[0])\n",
    "# show_batch(of_specs_ampl, None, size=20, title=emotion)\n",
    "# plt.imshow(of_spec)\n",
    "# plt.imshow(yf_spec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plotly.io as pio\n",
    "# pio.renderers.default = \"notebook\"\n",
    "# from plotly.offline import init_notebook_mode\n",
    "# init_notebook_mode(connected=True)\n",
    "\n",
    "# from utils import *\n",
    "# inference_b = get_inference_batch()\n",
    "# load_and_display_attn(\"utt_v.jepa_labeled.contrastive_0.0_layers.12\", inference_b, graph_title=\"unsupervised jepa 1.0 attn_weights\")\n",
    "# load_and_display_attn(\"utt_video.offset.contrastive.labeled0.0_backbone.out.sif\", inference_b, graph_title=\"unsupervised 0.0 attn_weights\")\n",
    "# load_and_display_attn(\"utt_video.offset.contrastive.labeled0.75\", graph_title=\"unsupervised 1.0 attn_weights\")\n",
    "# load_and_display_attn(\"utt_video.offset.contrastive.labeled0.1\", graph_title=\"unsupervised 1.0 attn_weights\")\n",
    "# load_and_display_attn(\"utt_video.offset.contrastive.labeled1.0\", inference_b, graph_title=\"unsupervised 1.0 attn_weights\")\n",
    "# load_and_display_attn(\"utt_video.offset.contrastive.labeled0.0_redux\", inference_b, graph_title=\"unsupervised 0.0 attn_weights\")\n",
    "# load_and_display_attn(\"utt_video.offset.contrastive.labeled0.5_redux\", inference_b, graph_title=\"unsupervised 0.5 attn_weights\")\n",
    "# load_and_display_attn(\"utt_video.offset.contrastive.labeled1.0_redux\", inference_b, graph_title=\"unsupervised 1.0 attn_weights\")\n",
    "# load_and_display_attn(\"utt_resnet.backbone_contrastive_1.0\", inference_b, graph_title=\"unsupervised 1.0 attn_weights\")\n",
    "# load_and_display_attn(\"utt_resnet.backbone_contrastive_0.0\", inference_b, graph_title=\"unsupervised 0.0 attn_weights\")\n",
    "\n",
    "# load_and_display_attn(\"utt_ast.audio_vit.video_contrastive.loss_labeled0.0\", inference_b, graph_title=\"unsupervised 0.0 attn_weights\")\n",
    "# load_and_display_attn(\"utt_ast_audio.only_contrastive.loss_labeled1.0\", inference_b, graph_title=\"unsupervised 1.0 attn_weights\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
